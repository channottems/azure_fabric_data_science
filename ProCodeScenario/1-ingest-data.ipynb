{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark","jupyter_kernel_name":null},"kernelspec":{"display_name":"Synapse PySpark","language":null,"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"dependencies":{"lakehouse":{"default_lakehouse":"7408d684-59c8-43b0-a48e-8573c51f703d","default_lakehouse_name":"churnLakehouseArkema","default_lakehouse_workspace_id":"bc4c156b-ef53-4b86-b4a1-73fa9871d2f8","known_lakehouses":[{"id":"7408d684-59c8-43b0-a48e-8573c51f703d"}]}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"},"enableDebugMode":false}}},"cells":[{"id":"652c150e-e033-49f6-aacc-b1e3a807711d","cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Part 1: Ingest data into a Microsoft Fabric lakehouse using Apache Spark\n","\n","In this tutorial, you'll ingest data into Fabric lakehouses in delta lake format. Some important terms to understand:\n","\n","* **Lakehouse** -- A lakehouse is a collection of files/folders/tables that represent a database over a data lake used by the Spark engine and SQL engine for big data processing and that includes enhanced capabilities for ACID transactions when using the open-source Delta formatted tables.\n","\n","* **Delta Lake** - Delta Lake is an open-source storage layer that brings ACID transactions, scalable metadata management, and batch and streaming data processing to Apache Spark. A Delta Lake table is a data table format that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata management."],"outputs":null},{"id":"ed1c494e","cell_type":"markdown","metadata":{},"source":["## Step 1\n","\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will be downloading data from a public blob, then storing the data in the lakehouse. \n","- **Make sure you [add a lakehouse](https://aka.ms/fabric/addlakehouse) to the notebook before running it. Failure to do so will result in an error.**"],"outputs":null},{"id":"10dee645-ca94-450f-b48c-a35da422793e","cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Step 2 Download dataset and upload to lakehouse"],"outputs":null},{"id":"ae3d4f32-55a2-45f0-bda6-19ac340b708c","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"ms_comment_ranges":{},"ms_comments":[],"nteract":{"transient":{"deleting":false}}},"source":["IS_CUSTOM_DATA = False  # if TRUE, dataset has to be uploaded manually\n","\n","DATA_ROOT = \"/lakehouse/default\"\n","DATA_FOLDER = \"Files/churn\"  # folder with data files\n","DATA_FILE = \"churn.csv\"  # data file name"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"ecadf6d2-eb30-4513-bfa1-b91443c41fde","normalized_state":"finished","queued_time":"2025-04-30T13:10:03.2476833Z","session_start_time":"2025-04-30T13:10:03.2485664Z","execution_start_time":"2025-04-30T13:10:15.9048687Z","execution_finish_time":"2025-04-30T13:10:16.2739648Z","parent_msg_id":"6f24848e-9c9b-46fe-809a-8fac83f81355"},"text/plain":"StatementMeta(, ecadf6d2-eb30-4513-bfa1-b91443c41fde, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1},{"id":"696ee4b1-dbea-4479-af62-69135be55634","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"source":["import os, requests\n","if not IS_CUSTOM_DATA:\n","# Using synapse blob, this can be done in one line\n","\n","# Download demo data files into lakehouse if not exist\n","    remote_url = \"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn\"\n","    file_list = [DATA_FILE]\n","    download_path = f\"{DATA_ROOT}/{DATA_FOLDER}/raw\"\n","\n","    if not os.path.exists(\"/lakehouse/default\"):\n","        raise FileNotFoundError(\n","            \"Default lakehouse not found, please add a lakehouse and restart the session.\"\n","        )\n","    os.makedirs(download_path, exist_ok=True)\n","    for fname in file_list:\n","        if not os.path.exists(f\"{download_path}/{fname}\"):\n","            r = requests.get(f\"{remote_url}/{fname}\", timeout=30)\n","            with open(f\"{download_path}/{fname}\", \"wb\") as f:\n","                f.write(r.content)\n","    print(\"Downloaded demo data files into lakehouse.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"ecadf6d2-eb30-4513-bfa1-b91443c41fde","normalized_state":"finished","queued_time":"2025-04-30T13:10:03.2514836Z","session_start_time":null,"execution_start_time":"2025-04-30T13:10:16.2761732Z","execution_finish_time":"2025-04-30T13:10:21.2878818Z","parent_msg_id":"9d0dc85e-4163-4913-b9d9-a8b96ced30d0"},"text/plain":"StatementMeta(, ecadf6d2-eb30-4513-bfa1-b91443c41fde, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloaded demo data files into lakehouse.\n"]}],"execution_count":2},{"id":"0b52c06b","cell_type":"markdown","metadata":{},"source":["## Next step\n","\n","You'll use the data you just ingested in [Part 2: Explore and cleanse data](https://learn.microsoft.com/fabric/data-science/tutorial-data-science-explore-notebook)."],"outputs":null}]}